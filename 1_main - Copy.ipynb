{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "players: 5347 rows, 13 cols\n",
      "  columns: ['nba_id', 'player_name', 'height', 'weight', 'dob', 'draft_year', 'draft_slot', 'position', 'country', 'current_team', 'active_roster', 'season', 'rookie_season']\n",
      "\n",
      "player_ratings: 1,089,331 rows, 66 cols\n",
      "  columns: ['nba_id', 'date', 'season', 'team_name', 'tm_id', 'future_game', 'active_roster', 'available', 'poss', 'dpm', 'o_dpm', 'd_dpm', 'box_dpm', 'box_odpm', 'box_ddpm', 'on_off_dpm', 'on_off_odpm', 'on_off_ddpm', 'age', 'career_game_num', 'seconds_played', 'position', 'position_num', 'x_position', 'bayes_rapm_off', 'bayes_rapm_def', 'bayes_rapm_total', 'rapm_exposure', 'x_minutes', 'x_pace', 'x_pts_100', 'x_ast_100', 'x_orb_100', 'x_drb_100', 'x_stl_100', 'x_blk_100', 'x_tov_100', 'x_fga_100', 'x_fg3a_100', 'x_fta_100', 'x_fg_pct', 'x_fg3_pct', 'x_ft_pct', 'tr_minutes', 'tr_starter', 'tr_fg3_pct', 'tr_ft_pct', 'projected_years_remaining', 'projected_years_remaining_cal', 'x_retirement_age', 'x_retirement_age_cal', 's1', 's2', 's3', 's4', 's5', 's6', 's7', 's8', 's9', 's10', 's11', 's12', 's13', 's14', 's15']\n",
      "  date range: 1996-11-01 to 2026-03-03\n",
      "  unique players: 2916\n",
      "  coverage — age: 1,089,331, rapm: 1,041,814, projections: 1,089,331, survivorship: 1,089,331\n",
      "\n",
      "Saved to c:\\Users\\kmedv\\OneDrive\\github\\darko\\supabase_tables/\n",
      "  players.parq: 0.1 MB\n",
      "  player_ratings.parq: 151.0 MB\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "\"\"\"\n",
    "Build Supabase-ready tables by joining DARKO pipeline parquet outputs.\n",
    "\n",
    "Produces:\n",
    "  1) players         — dimension, one row per player\n",
    "  2) player_ratings  — fact, one row per player per date\n",
    "\n",
    "Type conventions:\n",
    "  nba_id: Int64, date: Date, season: Int32, tm_id: Int32\n",
    "\n",
    "Uses lazy scans + semi-joins so we never eagerly load the 15.7M-row RAPM table.\n",
    "\"\"\"\n",
    "\n",
    "import polars as pl\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def find_project_root(start: Path) -> Path:\n",
    "    for p in [start, *start.parents]:\n",
    "        if (p / \"fixed_data\").exists() and (p / \"calculated_data\").exists():\n",
    "            return p\n",
    "    raise FileNotFoundError(\n",
    "        f\"Cannot find DARKO project root (fixed_data/ + calculated_data/). start={start}\"\n",
    "    )\n",
    "\n",
    "\n",
    "# %%\n",
    "# === CONFIG ===\n",
    "ROOT = find_project_root(Path.cwd())\n",
    "CALCULATED_DATA = ROOT / \"calculated_data\"\n",
    "TEMP_DIR = CALCULATED_DATA / \"temp\"\n",
    "OUTPUT_DIR = ROOT / \"supabase_tables\"\n",
    "OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# %%\n",
    "# === LAZY SCANS WITH TYPE CASTS ===\n",
    "\n",
    "# Base table — defines the grain (1,089,331 rows)\n",
    "spm = (\n",
    "    pl.scan_parquet(TEMP_DIR / \"spm_outputs.parq\")\n",
    "    .with_columns(\n",
    "        pl.col(\"nba_id\").cast(pl.Int64, strict=False),\n",
    "        pl.col(\"date\").cast(pl.Date),\n",
    "        pl.col(\"season\").cast(pl.Int32, strict=False),\n",
    "        pl.col(\"tm_id\").cast(pl.Int32, strict=False),\n",
    "        pl.col(\"future_game\").cast(pl.Int32, strict=False),\n",
    "        pl.col(\"active_roster\").cast(pl.Int8, strict=False),\n",
    "    )\n",
    ")\n",
    "\n",
    "# Bio columns from assembled features (column-projected, not full 500+ col load)\n",
    "bio = (\n",
    "    pl.scan_parquet(CALCULATED_DATA / \"5_assembled_features.parq\")\n",
    "    .select(\"nba_id\", \"date\", \"age\", \"career_game_num\", \"seconds_played\",\n",
    "            \"position\", \"position_num\", \"x_position\")\n",
    "    .with_columns(\n",
    "        pl.col(\"nba_id\").cast(pl.Int64, strict=False),\n",
    "        pl.col(\"date\").cast(pl.Date),\n",
    "        pl.col(\"career_game_num\").cast(pl.Int32, strict=False),\n",
    "        pl.col(\"seconds_played\").cast(pl.Int32, strict=False),\n",
    "    )\n",
    ")\n",
    "\n",
    "# RAPM (15.7M rows — filtered via semi-join before collecting)\n",
    "rapm = (\n",
    "    pl.scan_parquet(CALCULATED_DATA / \"bayes_rapm_ratings.parq\")\n",
    "    .select(\n",
    "        pl.col(\"nba_id\").cast(pl.Int64, strict=False),\n",
    "        pl.col(\"date\").cast(pl.Date),\n",
    "        pl.col(\"offense\").alias(\"bayes_rapm_off\"),\n",
    "        pl.col(\"defense\").alias(\"bayes_rapm_def\"),\n",
    "        pl.col(\"total\").alias(\"bayes_rapm_total\"),\n",
    "        pl.col(\"exposure\").alias(\"rapm_exposure\"),\n",
    "    )\n",
    ")\n",
    "\n",
    "# Projections (1,089,331 rows)\n",
    "projections = (\n",
    "    pl.scan_parquet(CALCULATED_DATA / \"talent_game_predictions.parq\")\n",
    "    .with_columns(\n",
    "        pl.col(\"nba_id\").cast(pl.Int64, strict=False),\n",
    "        pl.col(\"date\").cast(pl.Date),\n",
    "    )\n",
    "    .select(\n",
    "        \"nba_id\", \"date\",\n",
    "        \"x_minutes\", \"x_pace\",\n",
    "        \"x_pts_100\", \"x_ast_100\", \"x_orb_100\", \"x_drb_100\",\n",
    "        \"x_stl_100\", \"x_blk_100\", \"x_tov_100\",\n",
    "        \"x_fga_100\", \"x_fg3a_100\", \"x_fta_100\",\n",
    "        \"x_fg_pct\", \"x_fg3_pct\", \"x_ft_pct\",\n",
    "        \"tr_minutes\", \"tr_starter\", \"tr_fg3_pct\", \"tr_ft_pct\",\n",
    "    )\n",
    ")\n",
    "\n",
    "# Survivorship (1,084,268 rows, nba_id is Float64 in source)\n",
    "surv_rename = {str(t): f\"s{t}\" for t in range(1, 16)}\n",
    "survivorship = (\n",
    "    pl.scan_parquet(TEMP_DIR / \"nba_survivorship.parq\")\n",
    "    .with_columns(\n",
    "        pl.col(\"nba_id\").cast(pl.Int64, strict=False),\n",
    "        pl.col(\"date\").cast(pl.Date),\n",
    "    )\n",
    "    .select(\n",
    "        \"nba_id\", \"date\",\n",
    "        \"projected_years_remaining\", \"projected_years_remaining_cal\",\n",
    "        \"x_retirement_age\", \"x_retirement_age_cal\",\n",
    "        *[str(t) for t in range(1, 16)],\n",
    "    )\n",
    "    .rename(surv_rename)\n",
    ")\n",
    "\n",
    "# Crosswalk for dimension table\n",
    "crosswalk = (\n",
    "    pl.scan_csv(\n",
    "        ROOT / \"fixed_data\" / \"crosswalks\" / \"player_master_crosswalk.csv\",\n",
    "        infer_schema_length=0,\n",
    "    )\n",
    "    .select(\"nba_id\", \"player_name\", \"height\", \"weight\", \"dob\",\n",
    "            \"draft_year\", \"draft_slot\", \"position\", \"country\")\n",
    "    .with_columns(\n",
    "        pl.col(\"nba_id\").cast(pl.Int64, strict=False),\n",
    "        pl.col(\"height\").cast(pl.Float64, strict=False),\n",
    "        pl.col(\"weight\").cast(pl.Float64, strict=False),\n",
    "        pl.col(\"draft_year\").cast(pl.Int32, strict=False),\n",
    "        pl.col(\"draft_slot\").cast(pl.Int32, strict=False),\n",
    "    )\n",
    ")\n",
    "\n",
    "# %%\n",
    "# =============================================================================\n",
    "# TABLE 1: players (dimension)\n",
    "# =============================================================================\n",
    "\n",
    "# Deterministic \"latest\": max(date) per nba_id, then join back for fields\n",
    "spm_dim = spm.select(\"nba_id\", \"date\", \"team_name\", \"active_roster\", \"season\", \"player_name\")\n",
    "\n",
    "max_dates = spm_dim.group_by(\"nba_id\").agg(pl.max(\"date\").alias(\"max_date\"))\n",
    "\n",
    "latest = (\n",
    "    max_dates\n",
    "    .join(\n",
    "        spm_dim.select(\"nba_id\", \"date\", \"team_name\", \"active_roster\", \"season\"),\n",
    "        left_on=[\"nba_id\", \"max_date\"],\n",
    "        right_on=[\"nba_id\", \"date\"],\n",
    "        how=\"left\",\n",
    "    )\n",
    "    .select(\n",
    "        \"nba_id\",\n",
    "        pl.col(\"team_name\").alias(\"current_team\"),\n",
    "        \"active_roster\",\n",
    "        \"season\",\n",
    "    )\n",
    "    .unique(subset=\"nba_id\")\n",
    ")\n",
    "\n",
    "# Rookie season from survivorship (single scan, check schema first)\n",
    "surv_raw = pl.scan_parquet(TEMP_DIR / \"nba_survivorship.parq\").with_columns(\n",
    "    pl.col(\"nba_id\").cast(pl.Int64, strict=False),\n",
    ")\n",
    "has_rookie = \"rookie_season\" in surv_raw.collect_schema().names()\n",
    "rookie = (\n",
    "    surv_raw.select(\"nba_id\", \"rookie_season\").unique(subset=\"nba_id\")\n",
    "    if has_rookie else None\n",
    ")\n",
    "\n",
    "# Player name fallback: crosswalk may not have everyone, spm does\n",
    "name_from_spm = (\n",
    "    spm_dim\n",
    "    .group_by(\"nba_id\")\n",
    "    .agg(pl.first(\"player_name\").alias(\"player_name_spm\"))\n",
    ")\n",
    "\n",
    "# Union all nba_ids from fact + crosswalk so dimension covers everyone\n",
    "all_ids = (\n",
    "    pl.concat([spm.select(\"nba_id\"), crosswalk.select(\"nba_id\")], how=\"vertical\")\n",
    "    .unique(subset=\"nba_id\")\n",
    ")\n",
    "\n",
    "players_lf = (\n",
    "    all_ids\n",
    "    .join(crosswalk.unique(subset=\"nba_id\"), on=\"nba_id\", how=\"left\")\n",
    "    .join(name_from_spm, on=\"nba_id\", how=\"left\")\n",
    "    .join(latest, on=\"nba_id\", how=\"left\")\n",
    ")\n",
    "\n",
    "if rookie is not None:\n",
    "    players_lf = players_lf.join(rookie, on=\"nba_id\", how=\"left\")\n",
    "\n",
    "# Use crosswalk name, fall back to spm name\n",
    "players_lf = (\n",
    "    players_lf\n",
    "    .with_columns(\n",
    "        pl.coalesce([pl.col(\"player_name\"), pl.col(\"player_name_spm\")]).alias(\"player_name\")\n",
    "    )\n",
    "    .drop(\"player_name_spm\")\n",
    ")\n",
    "\n",
    "players = players_lf.collect()\n",
    "\n",
    "print(f\"players: {players.shape[0]} rows, {players.shape[1]} cols\")\n",
    "print(f\"  columns: {players.columns}\")\n",
    "\n",
    "# %%\n",
    "# =============================================================================\n",
    "# TABLE 2: player_ratings (fact)\n",
    "# =============================================================================\n",
    "\n",
    "df_base = spm.select(\n",
    "    \"nba_id\", \"date\", \"season\", \"team_name\", \"tm_id\",\n",
    "    \"future_game\", \"active_roster\", \"available\", \"poss\",\n",
    "    \"dpm\", \"o_dpm\", \"d_dpm\",\n",
    "    \"box_dpm\", \"box_odpm\", \"box_ddpm\",\n",
    "    \"on_off_dpm\", \"on_off_odpm\", \"on_off_ddpm\",\n",
    ")\n",
    "\n",
    "# Semi-join keys to filter large right-side tables before collecting\n",
    "keys = df_base.select(\"nba_id\", \"date\").unique()\n",
    "\n",
    "bio_f = bio.join(keys, on=[\"nba_id\", \"date\"], how=\"semi\")\n",
    "rapm_f = rapm.join(keys, on=[\"nba_id\", \"date\"], how=\"semi\")\n",
    "proj_f = projections.join(keys, on=[\"nba_id\", \"date\"], how=\"semi\")\n",
    "surv_f = survivorship.join(keys, on=[\"nba_id\", \"date\"], how=\"semi\")\n",
    "\n",
    "df_lf = (\n",
    "    df_base\n",
    "    .join(bio_f, on=[\"nba_id\", \"date\"], how=\"left\")\n",
    "    .join(rapm_f, on=[\"nba_id\", \"date\"], how=\"left\")\n",
    "    .join(proj_f, on=[\"nba_id\", \"date\"], how=\"left\")\n",
    "    .join(surv_f, on=[\"nba_id\", \"date\"], how=\"left\")\n",
    ")\n",
    "\n",
    "df = df_lf.collect()\n",
    "\n",
    "# %%\n",
    "# === VALIDATION ===\n",
    "\n",
    "dupes = df.select(\"nba_id\", \"date\").is_duplicated().sum()\n",
    "assert dupes == 0, f\"Duplicate (nba_id, date) rows: {dupes}\"\n",
    "\n",
    "base_rows = spm.select(pl.len()).collect().item()\n",
    "assert df.shape[0] == base_rows, (\n",
    "    f\"Row count changed: base={base_rows}, merged={df.shape[0]}. \"\n",
    "    \"A right-side table has duplicates on (nba_id, date).\"\n",
    ")\n",
    "\n",
    "def _nn(col: str) -> int:\n",
    "    return int(df.select(pl.col(col).is_not_null().sum()).item())\n",
    "\n",
    "print(f\"\\nplayer_ratings: {df.shape[0]:,} rows, {df.shape[1]} cols\")\n",
    "print(f\"  columns: {df.columns}\")\n",
    "print(f\"  date range: {df['date'].min()} to {df['date'].max()}\")\n",
    "print(f\"  unique players: {df['nba_id'].n_unique()}\")\n",
    "print(f\"  coverage — age: {_nn('age'):,}, rapm: {_nn('bayes_rapm_total'):,}, \"\n",
    "      f\"projections: {_nn('x_pts_100'):,}, survivorship: {_nn('projected_years_remaining'):,}\")\n",
    "\n",
    "# %%\n",
    "# === SAVE ===\n",
    "\n",
    "players.write_parquet(OUTPUT_DIR / \"players.parq\", compression=\"zstd\")\n",
    "df.write_parquet(OUTPUT_DIR / \"player_ratings.parq\", compression=\"zstd\")\n",
    "\n",
    "print(f\"\\nSaved to {OUTPUT_DIR}/\")\n",
    "print(f\"  players.parq: {(OUTPUT_DIR / 'players.parq').stat().st_size / 1e6:.1f} MB\")\n",
    "print(f\"  player_ratings.parq: {(OUTPUT_DIR / 'player_ratings.parq').stat().st_size / 1e6:.1f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected!\n",
      "\n",
      "=== players ===\n",
      "  Truncate + reload\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  COPY players: 100%|██████████| 1/1 [00:00<00:00,  1.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  5347 rows\n",
      "\n",
      "=== player_ratings ===\n",
      "  Source: 1,089,331 rows, 66 cols\n",
      "  Deleted 31,661 rows for season 2026\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  COPY player_ratings: 100%|██████████| 1/1 [00:07<00:00,  7.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Inserted 31,661 rows (atomic)\n",
      "\n",
      "=== season_sim ===\n",
      "  Truncate + reload\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  COPY season_sim: 100%|██████████| 1/1 [00:00<00:00,  5.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  30 rows\n",
      "\n",
      "=== win_distribution ===\n",
      "  Truncate + reload\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  COPY win_distribution: 100%|██████████| 1/1 [00:00<00:00,  3.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  525 rows\n",
      "\n",
      "=== Verification ===\n",
      "  players: 5,347 rows\n",
      "  player_ratings: 1,089,331 rows\n",
      "  season_sim: 30 rows\n",
      "  win_distribution: 525 rows\n",
      "  Latest date: 2026-03-03 00:00:00\n",
      "\n",
      "  Top 5 DPM:\n",
      "    Devin Booker              Phoenix Suns DPM: +2.30 (O: +3.11, D: -0.82)\n",
      "    Collin Gillespie          Phoenix Suns DPM: +1.46 (O: +0.84, D: +0.62)\n",
      "    Grayson Allen             Phoenix Suns DPM: +0.46 (O: +1.12, D: -0.66)\n",
      "    Oso Ighodaro              Phoenix Suns DPM: +0.22 (O: -1.67, D: +1.89)\n",
      "    Jordan Goodwin            Phoenix Suns DPM: +0.09 (O: -0.57, D: +0.66)\n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "\"\"\"\n",
    "Upload DARKO tables to Supabase (Postgres) using COPY.\n",
    "\n",
    "Auto-detects fresh vs update per table:\n",
    "  - Table missing  → CREATE + bulk load + indexes\n",
    "  - Table exists:\n",
    "      players:          TRUNCATE + reload\n",
    "      player_ratings:   DELETE current season + insert (single transaction)\n",
    "      season_sim:       TRUNCATE + reload\n",
    "      win_distribution: TRUNCATE + reload\n",
    "\n",
    "Connection: set SUPABASE_PG_DSN env var, e.g.:\n",
    "  export SUPABASE_PG_DSN=\"host=... port=5432 dbname=postgres user=... password=...\"\n",
    "\n",
    "Prereqs: run build_supabase_tables.py first.\n",
    "\"\"\"\n",
    "\n",
    "import io\n",
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import polars as pl\n",
    "import psycopg2\n",
    "from psycopg2 import sql as pgsql\n",
    "from tqdm import tqdm\n",
    "\n",
    "SCHEMA = \"public\"\n",
    "\n",
    "\n",
    "def find_project_root(start: Path) -> Path:\n",
    "    for p in [start, *start.parents]:\n",
    "        if (p / \"fixed_data\").exists() and (p / \"calculated_data\").exists():\n",
    "            return p\n",
    "    raise FileNotFoundError(\n",
    "        f\"Cannot find DARKO project root (fixed_data/ + calculated_data/). start={start}\"\n",
    "    )\n",
    "\n",
    "\n",
    "# %%\n",
    "# === CONFIG ===\n",
    "ROOT = find_project_root(Path.cwd())\n",
    "SUPABASE_TABLES = ROOT / \"supabase_tables\"\n",
    "CALCULATED_DATA = ROOT / \"calculated_data\"\n",
    "\n",
    "DSN = os.getenv(\"SUPABASE_PG_DSN\")\n",
    "if not DSN:\n",
    "    secrets_path = ROOT / \"fixed_data\" / \"supabase_secret.json\"\n",
    "    with open(secrets_path) as f:\n",
    "        _sec = json.load(f)\n",
    "    DSN = f\"host={_sec['host']} port={_sec['port']} dbname={_sec['dbname']} user={_sec['user']} password={_sec['password']}\"\n",
    "\n",
    "\n",
    "# %%\n",
    "# === HELPERS ===\n",
    "\n",
    "PG_TYPE_MAP = {\n",
    "    pl.Int8: \"SMALLINT\",\n",
    "    pl.Int16: \"SMALLINT\",\n",
    "    pl.Int32: \"INTEGER\",\n",
    "    pl.Int64: \"BIGINT\",\n",
    "    pl.Float32: \"REAL\",\n",
    "    pl.Float64: \"DOUBLE PRECISION\",\n",
    "    pl.Boolean: \"BOOLEAN\",\n",
    "    pl.Utf8: \"TEXT\",\n",
    "    pl.Categorical: \"TEXT\",\n",
    "    pl.Date: \"DATE\",\n",
    "}\n",
    "\n",
    "\n",
    "def get_conn():\n",
    "    conn = psycopg2.connect(DSN)\n",
    "    conn.autocommit = False\n",
    "    return conn\n",
    "\n",
    "\n",
    "def table_exists(conn, table_name: str) -> bool:\n",
    "    with conn.cursor() as cur:\n",
    "        cur.execute(\n",
    "            \"SELECT EXISTS (SELECT 1 FROM information_schema.tables \"\n",
    "            \"WHERE table_schema = %s AND table_name = %s)\",\n",
    "            (SCHEMA, table_name),\n",
    "        )\n",
    "        return cur.fetchone()[0]\n",
    "\n",
    "\n",
    "def create_table(conn, df: pl.DataFrame, table_name: str):\n",
    "    \"\"\"CREATE TABLE if it doesn't exist. Does NOT drop existing tables.\"\"\"\n",
    "    col_defs = []\n",
    "    for name, dtype in df.schema.items():\n",
    "        if dtype in PG_TYPE_MAP:\n",
    "            pg_type = PG_TYPE_MAP[dtype]\n",
    "        elif isinstance(dtype, pl.Datetime):\n",
    "            pg_type = \"TIMESTAMP\"\n",
    "        else:\n",
    "            pg_type = \"TEXT\"\n",
    "        col_defs.append(\n",
    "            pgsql.SQL(\"{} {}\").format(pgsql.Identifier(name), pgsql.SQL(pg_type))\n",
    "        )\n",
    "\n",
    "    stmt = pgsql.SQL(\"CREATE TABLE {}.{} ({})\").format(\n",
    "        pgsql.Identifier(SCHEMA),\n",
    "        pgsql.Identifier(table_name),\n",
    "        pgsql.SQL(\", \").join(col_defs),\n",
    "    )\n",
    "    with conn.cursor() as cur:\n",
    "        cur.execute(stmt)\n",
    "\n",
    "\n",
    "def copy_df(conn, df: pl.DataFrame, table_name: str, chunksize: int = 50_000):\n",
    "    \"\"\"Bulk upload polars DataFrame via COPY FROM STDIN (CSV).\"\"\"\n",
    "    cols_sql = pgsql.SQL(\", \").join(pgsql.Identifier(c) for c in df.columns)\n",
    "    copy_stmt = pgsql.SQL(\n",
    "        \"COPY {}.{} ({}) FROM STDIN WITH (FORMAT csv, NULL '\\\\N')\"\n",
    "    ).format(pgsql.Identifier(SCHEMA), pgsql.Identifier(table_name), cols_sql)\n",
    "    copy_str = copy_stmt.as_string(conn)\n",
    "\n",
    "    n = df.shape[0]\n",
    "    n_chunks = (n + chunksize - 1) // chunksize\n",
    "\n",
    "    with conn.cursor() as cur:\n",
    "        for i in tqdm(range(0, n, chunksize), total=n_chunks, desc=f\"  COPY {table_name}\"):\n",
    "            chunk = df.slice(i, chunksize)\n",
    "            buf = io.BytesIO()\n",
    "            chunk.write_csv(buf, include_header=False, null_value=\"\\\\N\")\n",
    "            buf.seek(0)\n",
    "            cur.copy_expert(copy_str, buf)\n",
    "\n",
    "\n",
    "def execute(conn, stmt: str, params=None):\n",
    "    with conn.cursor() as cur:\n",
    "        cur.execute(stmt, params)\n",
    "        return cur.rowcount\n",
    "\n",
    "\n",
    "def qualified(table_name: str) -> str:\n",
    "    return f\"{SCHEMA}.{table_name}\"\n",
    "\n",
    "\n",
    "# %%\n",
    "# === TEST CONNECTION ===\n",
    "with get_conn() as conn:\n",
    "    execute(conn, \"SELECT 1\")\n",
    "    conn.commit()\n",
    "print(\"Connected!\")\n",
    "\n",
    "# %%\n",
    "# =============================================================================\n",
    "# players\n",
    "# =============================================================================\n",
    "print(\"\\n=== players ===\")\n",
    "\n",
    "players = pl.read_parquet(SUPABASE_TABLES / \"players.parq\")\n",
    "\n",
    "with get_conn() as conn:\n",
    "    if not table_exists(conn, \"players\"):\n",
    "        print(\"  Creating fresh\")\n",
    "        create_table(conn, players, \"players\")\n",
    "        copy_df(conn, players, \"players\")\n",
    "        execute(conn, f\"ALTER TABLE {qualified('players')} ADD PRIMARY KEY (nba_id)\")\n",
    "    else:\n",
    "        print(\"  Truncate + reload\")\n",
    "        execute(conn, f\"TRUNCATE TABLE {qualified('players')}\")\n",
    "        copy_df(conn, players, \"players\")\n",
    "    conn.commit()\n",
    "\n",
    "print(f\"  {players.shape[0]} rows\")\n",
    "\n",
    "# %%\n",
    "# =============================================================================\n",
    "# player_ratings\n",
    "# =============================================================================\n",
    "print(\"\\n=== player_ratings ===\")\n",
    "\n",
    "df = pl.read_parquet(SUPABASE_TABLES / \"player_ratings.parq\")\n",
    "print(f\"  Source: {df.shape[0]:,} rows, {df.shape[1]} cols\")\n",
    "\n",
    "with get_conn() as conn:\n",
    "    if not table_exists(conn, \"player_ratings\"):\n",
    "        print(\"  Creating fresh\")\n",
    "        create_table(conn, df, \"player_ratings\")\n",
    "        copy_df(conn, df, \"player_ratings\")\n",
    "        execute(conn, f\"ALTER TABLE {qualified('player_ratings')} \"\n",
    "                       \"ADD CONSTRAINT pk_player_ratings PRIMARY KEY (nba_id, date)\")\n",
    "        execute(conn, f\"CREATE INDEX idx_ratings_date ON {qualified('player_ratings')} (date DESC)\")\n",
    "        execute(conn, f\"CREATE INDEX idx_ratings_season ON {qualified('player_ratings')} (season)\")\n",
    "        execute(conn, f\"CREATE INDEX idx_ratings_nba_id ON {qualified('player_ratings')} (nba_id)\")\n",
    "        conn.commit()\n",
    "        print(f\"  {df.shape[0]:,} rows loaded, indexes created\")\n",
    "    else:\n",
    "        # Atomic: delete current season + insert in one transaction\n",
    "        current_season = int(df[\"season\"].max())\n",
    "        current_season_df = df.filter(pl.col(\"season\") == current_season)\n",
    "\n",
    "        deleted = execute(conn, f\"DELETE FROM {qualified('player_ratings')} WHERE season = %s\",\n",
    "                          (current_season,))\n",
    "        print(f\"  Deleted {deleted:,} rows for season {current_season}\")\n",
    "\n",
    "        copy_df(conn, current_season_df, \"player_ratings\")\n",
    "        conn.commit()\n",
    "        print(f\"  Inserted {current_season_df.shape[0]:,} rows (atomic)\")\n",
    "\n",
    "# %%\n",
    "# =============================================================================\n",
    "# season_sim\n",
    "# =============================================================================\n",
    "print(\"\\n=== season_sim ===\")\n",
    "\n",
    "season_sim = pl.read_csv(CALCULATED_DATA / \"season_sim.csv\")\n",
    "\n",
    "with get_conn() as conn:\n",
    "    if not table_exists(conn, \"season_sim\"):\n",
    "        print(\"  Creating fresh\")\n",
    "        create_table(conn, season_sim, \"season_sim\")\n",
    "    else:\n",
    "        print(\"  Truncate + reload\")\n",
    "        execute(conn, f\"TRUNCATE TABLE {qualified('season_sim')}\")\n",
    "    copy_df(conn, season_sim, \"season_sim\")\n",
    "    conn.commit()\n",
    "\n",
    "print(f\"  {season_sim.shape[0]} rows\")\n",
    "\n",
    "# %%\n",
    "# =============================================================================\n",
    "# win_distribution\n",
    "# =============================================================================\n",
    "print(\"\\n=== win_distribution ===\")\n",
    "\n",
    "win_dist = pl.read_parquet(CALCULATED_DATA / \"win_distribution.parq\")\n",
    "\n",
    "with get_conn() as conn:\n",
    "    if not table_exists(conn, \"win_distribution\"):\n",
    "        print(\"  Creating fresh\")\n",
    "        create_table(conn, win_dist, \"win_distribution\")\n",
    "    else:\n",
    "        print(\"  Truncate + reload\")\n",
    "        execute(conn, f\"TRUNCATE TABLE {qualified('win_distribution')}\")\n",
    "    copy_df(conn, win_dist, \"win_distribution\")\n",
    "    conn.commit()\n",
    "\n",
    "print(f\"  {win_dist.shape[0]} rows\")\n",
    "\n",
    "# %%\n",
    "# =============================================================================\n",
    "# VERIFY\n",
    "# =============================================================================\n",
    "print(\"\\n=== Verification ===\")\n",
    "\n",
    "with get_conn() as conn:\n",
    "    with conn.cursor() as cur:\n",
    "        for tbl in [\"players\", \"player_ratings\", \"season_sim\", \"win_distribution\"]:\n",
    "            cur.execute(f\"SELECT COUNT(*) FROM {qualified(tbl)}\")\n",
    "            print(f\"  {tbl}: {cur.fetchone()[0]:,} rows\")\n",
    "\n",
    "        cur.execute(f\"SELECT MAX(date) FROM {qualified('player_ratings')}\")\n",
    "        print(f\"  Latest date: {cur.fetchone()[0]}\")\n",
    "\n",
    "        cur.execute(f\"\"\"\n",
    "            SELECT pr.nba_id, p.player_name, pr.team_name, pr.dpm, pr.o_dpm, pr.d_dpm\n",
    "            FROM {qualified('player_ratings')} pr\n",
    "            JOIN {qualified('players')} p ON pr.nba_id = p.nba_id\n",
    "            WHERE pr.date = (SELECT MAX(date) FROM {qualified('player_ratings')})\n",
    "              AND pr.active_roster = 1\n",
    "            ORDER BY pr.dpm DESC\n",
    "            LIMIT 5\n",
    "        \"\"\")\n",
    "        print(\"\\n  Top 5 DPM:\")\n",
    "        for row in cur.fetchall():\n",
    "            print(f\"    {row[1]:25s} {row[2]:5s} DPM: {row[3]:+.2f} (O: {row[4]:+.2f}, D: {row[5]:+.2f})\")\n",
    "\n",
    "    conn.commit()\n",
    "\n",
    "print(\"\\nDone!\")"
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "darko311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
